{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c87a1e",
   "metadata": {},
   "source": [
    "**WEB SCRAPING FOR LEAD GENERATION**\n",
    "\n",
    "Hunter Luckow<br>\n",
    "Department of Computer Science & Innovation<br>\n",
    "Long Island University<br>\n",
    "2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121452be",
   "metadata": {},
   "source": [
    "# Using Selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1034eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.linkedin.com/jobs/search?keywords=Accountant&location=United%20States&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i <= 76:\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    print(f'# {i} executed')\n",
    "    i=i+1\n",
    "    \n",
    "    try:\n",
    "        x = driver.find_element('xpath', \"//button[@aria-label='See more jobs']\")\n",
    "        x.click()\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "        time.sleep(3)\n",
    "soupy = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6904db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LINKEDIN JOB POSTING WEBSCRAPE 1\n",
    "\n",
    "# Set target words to look for in the Description\n",
    "KEYWORDS = [\"NetSuite|netsuite|Netsuite|Net Suite\", # soft\n",
    "            \"QuickBooks|quickbooks|Quickbooks|Quick Books\", # soft\n",
    "            \"Sage Intacct|sage intacct|SAGE INTAACT|SageIntacct\", # soft\n",
    "            \"Xero|xero|XERO\", # soft\n",
    "            \"divvy|Divvy|DIVVY\", # comp\n",
    "            \"concur|Concur|CONCUR\", # comp\n",
    "            \"Expensify|expensify|EXPENSIFY\", # comp\n",
    "            \"Bill.com|bill.com\", #comp\n",
    "            \"Corpay|corpay|CorPay\"] # comp\n",
    "\n",
    "\n",
    "# Use BeautifulSoup to retrieve individual job cards data\n",
    "BASE_CARDS = soupy.find_all('div', class_='base-card')\n",
    "numPostings = len(BASE_CARDS)\n",
    "JOB_TITLES = soupy.find_all('h3', class_=\"base-search-card__title\")\n",
    "link_data = soupy.find_all('a', class_='base-card__full-link')\n",
    "LINKS = []\n",
    "for a in link_data:\n",
    "    link = a['href']\n",
    "    LINKS.append(link)\n",
    "COMPANIES = []\n",
    "for card in BASE_CARDS:\n",
    "    company = card.find('a', class_=\"hidden-nested-link\")\n",
    "    COMPANIES.append(company.string.strip())\n",
    "    \n",
    "df=[]\n",
    "for i in range(numPostings):\n",
    "    responseTEMP = requests.get(LINKS[i])\n",
    "    soup2 = BeautifulSoup(responseTEMP.text, \"html.parser\")\n",
    "    entry=[]\n",
    "    # Locate, Extract, convert job description data\n",
    "    jobDescription = str(soup2.find('script', type=\"application/ld+json\"))\n",
    "    for word in KEYWORDS:\n",
    "        if re.search(word, jobDescription):\n",
    "            match = re.search(word, jobDescription).group()\n",
    "            print(f\"The string contains the keyword: {match}\")\n",
    "            job_title = JOB_TITLES[i].string.strip()\n",
    "            entry.append(job_title)\n",
    "            company_name = COMPANIES[i]\n",
    "            entry.append(company_name)\n",
    "            jp_link = LINKS[i]\n",
    "            entry.append(jp_link)\n",
    "            entry.append(match)\n",
    "            df.append(entry)\n",
    "            break\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads=pd.DataFrame(df, columns=['Job Title', 'Company', 'LINK', 'Software Flagged'])\n",
    "leads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=leads['Company'].value_counts() \n",
    "vc[vc>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads_NoDups = leads.drop_duplicates(subset='Company')\n",
    "leads_NoDups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83410919",
   "metadata": {},
   "source": [
    "# Convert Final Output to Excel & CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add9f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leads_NoDups.to_excel('leads_NoDups.xlsx', index=False)\n",
    "#leads_NoDups.to_csv('leads_NoDups.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
